{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0a76df",
   "metadata": {},
   "source": [
    "# Restaurant Classifier \n",
    "#### Lila Kosowsky and Sarah Moore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51c4ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7ffea1aa48b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.9/site-packages/nltk/data.py\", line 1160, in __del__\n",
      "    if not self.closed:\n",
      "  File \"/Applications/anaconda3/lib/python3.9/site-packages/nltk/data.py\", line 1180, in closed\n",
      "    return self.stream.closed\n",
      "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lilakosowsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lilakosowsky/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import random \n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "38f6d419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##load pretrained word2vec model \n",
    "filename = '/Users/lilakosowsky/Desktop/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3de1d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import data\n",
    "data_file = '/Users/lilakosowsky/Desktop/Neural Nets/TA_restaurants_curated.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "##isolate the data we will be using \n",
    "data = data[['Name', 'Cuisine Style']]\n",
    "\n",
    "##drop rows with null values \n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "##drop rows with non-ascii characters \n",
    "for index, row in data.iterrows():\n",
    "    if not all(ord(c) < 128 for c in str(row)):\n",
    "        data.drop(index, inplace=True)\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d50a44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Mediterranean'       4904\n",
      "'Pizza'               3627\n",
      "'Bar'                 2849\n",
      "'British'             2550\n",
      "'Asian'               2512\n",
      "'Spanish'             1794\n",
      "'Central European'    1471\n",
      "'Cafe'                1467\n",
      "'Sushi'               1309\n",
      "'Fast Food'           1246\n",
      "'Pub'                 1011\n",
      "'Middle Eastern'       726\n",
      "'Thai'                 717\n",
      "'Italian'              633\n",
      "'French'               563\n",
      "'American'             550\n",
      "'Indian'               150\n",
      "'Chinese'              136\n",
      "Name: first_cuisine, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##makes sure that only the selected cuisines are included\n",
    "def filterCuisines(entry):\n",
    "    new_cuisines = []\n",
    "    tags = entry.split(\", \")\n",
    "    for tag in tags:\n",
    "        if tag in selected_cuisines:\n",
    "            new_cuisines.append(tag)\n",
    "    if not new_cuisines:\n",
    "        return None \n",
    "    return new_cuisines\n",
    "\n",
    "def preprocess(entry):\n",
    "    ##remove punctuation\n",
    "    entry = re.sub(r'[^\\w\\s]', '', entry)\n",
    "    \n",
    "    ##convert to lowercase\n",
    "    entry = entry.lower()\n",
    "    \n",
    "    ##remove stopwords \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    entry = ' '.join([word for word in entry.split() if word not in stopwords])\n",
    "    return entry\n",
    "\n",
    "def tokenize(entry):\n",
    "    ##tokenize into individual words \n",
    "    words = nltk.word_tokenize(entry)\n",
    "    return words \n",
    "\n",
    "##gets vector of each word from google word2vec model\n",
    "def vectorize(entry):\n",
    "    vector = []\n",
    "    for word in entry:\n",
    "        if word in model.key_to_index.keys():\n",
    "            if model[word] is not None:\n",
    "                vector.append(model[word])\n",
    "    return vector\n",
    "\n",
    "#takes average of all word vectors in an entry\n",
    "def average(entry):\n",
    "    if len(entry) != 0: \n",
    "        return sum(entry) / len(entry)\n",
    "\n",
    "##returns only the first word of an entry\n",
    "def getFirst(entry):\n",
    "        return entry[0]\n",
    "\n",
    "\n",
    "##top 21 cuisines (filtered out generic descriptors such as \"vegan friendly\")\n",
    "selected_cuisines = [\"'Mediterranean'\", \"'Italian'\", \"'Bar'\", \"'French'\", \"'Asian'\", \"'Pizza'\", \"'Spanish'\", \"'Pub'\", \"'Cafe'\", \"'Fast Food'\", \"'British'\", \"'Central European'\", \"'Chinese'\", \"'Sushi'\", \"'American'\", \"'Portugese'\", \"'Indian'\", \"'Middle Eastern'\", \"'Thai'\"]\n",
    "\n",
    "##filter out extra cuisines\n",
    "data['filtered_cuisines'] = data['Cuisine Style'].apply(filterCuisines)\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "##isolate the first word for the restaurant names \n",
    "data['first_cuisine'] = data['filtered_cuisines'].apply(getFirst)\n",
    "\n",
    "print(data['first_cuisine'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c5a49487",
   "metadata": {},
   "outputs": [],
   "source": [
    "##process data\n",
    "data['cuisines_processed'] = data['first_cuisine'].apply(preprocess)\n",
    "data['names_processed'] = data['Name'].apply(preprocess)\n",
    "\n",
    "##tokenize data\n",
    "data['cuisines_tokenized'] = data['cuisines_processed'].apply(tokenize)\n",
    "data['names_tokenized'] = data['names_processed'].apply(tokenize)\n",
    "\n",
    "##get vector values for data\n",
    "data['cuisine_vector'] = data['cuisines_tokenized'].apply(vectorize)\n",
    "data['name_vector'] = data['names_tokenized'].apply(vectorize)\n",
    "\n",
    "##get average vector value for the cuisine types\n",
    "data['average_cuisine'] = data['cuisine_vector'].apply(average)\n",
    "data['average_name'] = data['name_vector'].apply(average)\n",
    "\n",
    "##filter out and last null values\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "##split into X and y\n",
    "X = data['average_name']\n",
    "y = data['average_cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1aa91cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "678/678 [==============================] - 21s 29ms/step - loss: 0.0267 - accuracy: 0.1282\n",
      "Epoch 2/25\n",
      "678/678 [==============================] - 25s 37ms/step - loss: 0.0259 - accuracy: 0.1490\n",
      "Epoch 3/25\n",
      "678/678 [==============================] - 25s 36ms/step - loss: 0.0257 - accuracy: 0.1576\n",
      "Epoch 4/25\n",
      "678/678 [==============================] - 24s 35ms/step - loss: 0.0255 - accuracy: 0.1632\n",
      "Epoch 5/25\n",
      "678/678 [==============================] - 24s 35ms/step - loss: 0.0254 - accuracy: 0.1663\n",
      "Epoch 6/25\n",
      "678/678 [==============================] - 28s 41ms/step - loss: 0.0254 - accuracy: 0.1685\n",
      "Epoch 7/25\n",
      "678/678 [==============================] - 25s 37ms/step - loss: 0.0253 - accuracy: 0.1687\n",
      "Epoch 8/25\n",
      "678/678 [==============================] - 27s 40ms/step - loss: 0.0253 - accuracy: 0.1751\n",
      "Epoch 9/25\n",
      "678/678 [==============================] - 26s 38ms/step - loss: 0.0252 - accuracy: 0.1725\n",
      "Epoch 10/25\n",
      "678/678 [==============================] - 29s 43ms/step - loss: 0.0252 - accuracy: 0.1709\n",
      "Epoch 11/25\n",
      "678/678 [==============================] - 28s 42ms/step - loss: 0.0251 - accuracy: 0.1798\n",
      "Epoch 12/25\n",
      "678/678 [==============================] - 24s 36ms/step - loss: 0.0251 - accuracy: 0.1818\n",
      "Epoch 13/25\n",
      "678/678 [==============================] - 26s 39ms/step - loss: 0.0251 - accuracy: 0.1770\n",
      "Epoch 14/25\n",
      "678/678 [==============================] - 22s 33ms/step - loss: 0.0250 - accuracy: 0.1851\n",
      "Epoch 15/25\n",
      "678/678 [==============================] - 24s 35ms/step - loss: 0.0250 - accuracy: 0.1819\n",
      "Epoch 16/25\n",
      "678/678 [==============================] - 24s 35ms/step - loss: 0.0250 - accuracy: 0.1873\n",
      "Epoch 17/25\n",
      "678/678 [==============================] - 23s 34ms/step - loss: 0.0249 - accuracy: 0.1842\n",
      "Epoch 18/25\n",
      "678/678 [==============================] - 24s 36ms/step - loss: 0.0249 - accuracy: 0.1848\n",
      "Epoch 19/25\n",
      "678/678 [==============================] - 23s 34ms/step - loss: 0.0249 - accuracy: 0.1839\n",
      "Epoch 20/25\n",
      "678/678 [==============================] - 22s 33ms/step - loss: 0.0249 - accuracy: 0.1867\n",
      "Epoch 21/25\n",
      "678/678 [==============================] - 22s 32ms/step - loss: 0.0249 - accuracy: 0.1871\n",
      "Epoch 22/25\n",
      "678/678 [==============================] - 22s 32ms/step - loss: 0.0249 - accuracy: 0.1903\n",
      "Epoch 23/25\n",
      "678/678 [==============================] - 23s 34ms/step - loss: 0.0248 - accuracy: 0.1919\n",
      "Epoch 24/25\n",
      "678/678 [==============================] - 22s 32ms/step - loss: 0.0248 - accuracy: 0.1935\n",
      "Epoch 25/25\n",
      "678/678 [==============================] - 21s 31ms/step - loss: 0.0248 - accuracy: 0.1898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffdd1a1df70>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##split into test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 123)\n",
    "\n",
    "##create model \n",
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, input_dim = 300, activation='ReLU'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(900, activation='ReLU'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(750, activation='ReLU'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(500, activation='ReLU'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(300, activation='ReLU')) \n",
    "model2.compile(optimizer='adam',\n",
    "             loss='mse', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "##reshape input data\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "y_train = np.array(y_train.tolist())\n",
    "y_train = y_train.reshape(y_train.shape[0], 300) # reshape the output to match the new dimension\n",
    "\n",
    "##train model\n",
    "model2.fit(X_train, y_train, epochs=25, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fdc7710b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258/2258 [==============================] - 6s 3ms/step - loss: 0.0251 - accuracy: 0.1880\n",
      "0.02513277530670166\n",
      "0.18802055716514587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, re_lu_87_layer_call_fn, re_lu_87_layer_call_and_return_conditional_losses, re_lu_88_layer_call_fn, re_lu_88_layer_call_and_return_conditional_losses while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model2/assets\n"
     ]
    }
   ],
   "source": [
    "##reshape test data\n",
    "X_test = np.array(X_test.tolist())\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "y_test = np.array(y_test.tolist())\n",
    "y_test = y_test.reshape(y_test.shape[0], 300) # reshape the output to match the new dimension\n",
    "\n",
    "##evaluate model \n",
    "score = model2.evaluate(X_test, y_test, batch_size=5)\n",
    "print(score[0])\n",
    "print(score[1])\n",
    "\n",
    "##save model\n",
    "model2.save('my_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c63ac705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South End Pita\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "[('mediterranean', 1.0), ('Mediterranean', 0.6283091902732849), ('Mediterrean', 0.582373321056366), ('Mediterannean', 0.5725836157798767), ('Mediteranean', 0.5462020635604858), ('Balkan_peninsula', 0.5421025156974792), ('Meditteranean', 0.5334994792938232), ('Mediterranean_basin', 0.527664303779602), ('Monemvasia', 0.5249188542366028), ('Tyrrhenian', 0.5243752598762512)]\n"
     ]
    }
   ],
   "source": [
    "##Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tensorflow.random.set_seed(42)\n",
    "\n",
    "##load saved model\n",
    "model2 = keras.models.load_model('my_model2')\n",
    "\n",
    "##plug in new restaurant name\n",
    "restaurant_name = 'South End Pita'\n",
    "\n",
    "def getPrediction(new_data):\n",
    "    processed_data = preprocess(new_data)\n",
    "    tokenized_data = tokenize(processed_data)\n",
    "    vectorized_data = vectorize(tokenized_data)\n",
    "    average_data = average(vectorized_data)\n",
    "    if average_data is None:\n",
    "        print(\"restaurant name not found in word2vec model\")\n",
    "        return\n",
    "    else:\n",
    "        input_data = tensorflow.reshape(average_data, (1, 300))\n",
    "        ##process name to plug into the model \n",
    "        input_data = process(restaurant_name)\n",
    "\n",
    "        ##get prediction from model\n",
    "        prediction = model2.predict(input_data)\n",
    "\n",
    "        ##process prediction to plug into word2vec model\n",
    "        cuisine = tensorflow.reshape(prediction,(300,))\n",
    "        cuisine = cuisine.numpy()\n",
    "\n",
    "        ##get most similar cuisine\n",
    "        similarities = y.apply(lambda y: np.dot(y, cuisine) / (np.linalg.norm(y) * np.linalg.norm(cuisine)))\n",
    "\n",
    "        ##Print out the most similar word\n",
    "        print(model.most_similar(y[similarities.idxmax()]))\n",
    "        return\n",
    "\n",
    "print(restaurant_name)\n",
    "getPrediction(restaurant_name)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
